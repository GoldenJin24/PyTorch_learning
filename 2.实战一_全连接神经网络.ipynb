{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "print(torch.__version__)\n",
    "# 台式机是pytorch1，有cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一个-全连接神经网络-判断矩阵“胖瘦”即高宽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2420])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2957]), tensor([0.9372]), 0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rectangle():\n",
    "    width = torch.rand(1)\n",
    "    height = torch.rand(1)\n",
    "    fat = int(height < width)\n",
    "    return width,height,fat\n",
    "get_rectangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, (tensor([0.9892, 0.2312]), 1))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.定义数据集\n",
    "# 注意大小写\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return 500\n",
    "    def __getitem__(self,index):\n",
    "        width,height,fat = get_rectangle()\n",
    "        # x 是一维的   有两个数据\n",
    "        X = torch.FloatTensor([width,height])\n",
    "        # X是tensor ， 但是y不是\n",
    "        Y = fat\n",
    "        return X,Y\n",
    "dataset = DataSet()\n",
    "len(dataset),dataset[1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, torch.Size([8, 2]), torch.Size([8]))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.数据集加载器\n",
    "# 有些英语单词是连着的，有些英语单词又用 _ 分割\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset,batch_size = 8, shuffle =True, drop_last = True)\n",
    "# loader自动会将数据变成tensor \n",
    "len(loader),next(iter(loader))[0].shape,next(iter(loader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.全连接神经网络 分类任务\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        #一定要写继承\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2,32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,2),\n",
    "            #softmax  \n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "model(torch.randn(8,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 2])\n",
      "Output shape: torch.Size([8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4976, 0.5024],\n",
       "        [0.4993, 0.5007],\n",
       "        [0.5131, 0.4869],\n",
       "        [0.4665, 0.5335],\n",
       "        [0.4769, 0.5231],\n",
       "        [0.4901, 0.5099],\n",
       "        [0.5049, 0.4951],\n",
       "        [0.5149, 0.4851]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分析一下\n",
    "model = Model()\n",
    "def get_shape(module, input, output):\n",
    "    print(f\"Input shape: {input[0].shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "# 注册forward hook来获取每一层的输入和输出形状变化\n",
    "for name, layer in model.named_children():\n",
    "    #print(name,layer)\n",
    "    #sequential 默认是一层\n",
    "    layer.register_forward_hook(get_shape)\n",
    "\n",
    "# 开始打印每一层的变化\n",
    "model(torch.randn(8,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7242922782897949 0.25\n",
      "20 0.5402665734291077 1.0\n",
      "40 0.42121559381484985 0.875\n",
      "60 0.3642852306365967 1.0\n",
      "80 0.31795355677604675 1.0\n"
     ]
    }
   ],
   "source": [
    "# 4.训练\n",
    "model = Model()\n",
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "    loss_fun = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        for i,(x,y) in enumerate(loader):\n",
    "            out = model(x)\n",
    "           \n",
    "            loss = loss_fun(out,y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch%20 == 0:\n",
    "            #都是训练集的结果，其实acc和loss.item()代表的含义差不多\n",
    "            acc = (out.argmax(dim=1) == y).sum().item() / len(y)\n",
    "            #item只能取一个元素的\n",
    "            print(epoch,loss.item(),acc)\n",
    "    torch.save(model,'./test1_model')\n",
    "\n",
    "#开始训练\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9907e-01, 9.2597e-04],\n",
      "        [9.9940e-01, 6.0503e-04],\n",
      "        [9.9912e-01, 8.7973e-04],\n",
      "        [7.0180e-05, 9.9993e-01],\n",
      "        [9.9819e-01, 1.8142e-03],\n",
      "        [9.9964e-01, 3.5509e-04],\n",
      "        [1.6649e-03, 9.9834e-01],\n",
      "        [9.9937e-01, 6.3114e-04]])\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 0])\n",
      "tensor([True, True, True, True, True, True, True, True])\n",
      "0.3137291371822357\n"
     ]
    }
   ],
   "source": [
    "# 5.测试\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    \n",
    "    #先加载模型，当然要是从头到尾跑的那也可以\n",
    "    model = torch.load('./test1_model')\n",
    "\n",
    "    #进入测试模式\n",
    "    model.eval()\n",
    "\n",
    "    #获取一批数据\n",
    "    x,y = next(iter(loader))\n",
    "    out = model(x)\n",
    "    print(out)\n",
    "    print(y)\n",
    "    print(out.argmax(dim=1) == y)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    loss = criterion(out, y)\n",
    "    print(loss.item())\n",
    "    #很奇怪，下面这样连这写是错误的\n",
    "    #print(torch.nn.CrossEntropyLoss(out,y).item())\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二个-计算矩阵的面积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5198]), tensor([0.4294]), tensor([0.2232]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rectangle():\n",
    "    width = torch.rand(1)\n",
    "    height = torch.rand(1)\n",
    "    s = width * height\n",
    "    return width,height,s\n",
    "get_rectangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, (tensor([0.5404, 0.4207]), tensor([0.2274])))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.定义数据集\n",
    "# 注意大小写\n",
    "class DataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return 500\n",
    "    def __getitem__(self,index):\n",
    "        width,height,s = get_rectangle()\n",
    "        # x 是一维的   有两个数据\n",
    "        X = torch.FloatTensor([width,height])\n",
    "        # X是tensor ， 但是y不是\n",
    "        # loader默认会转为tensor，不用担心\n",
    "        Y = torch.FloatTensor(s)\n",
    "        return X,Y\n",
    "dataset = DataSet()\n",
    "len(dataset),dataset[1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, torch.Size([8, 2]), torch.Size([8, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.数据集加载器\n",
    "# 有些英语单词是连着的，有些英语单词又用 _ 分割\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset,batch_size = 8, shuffle =True, drop_last = True)\n",
    "# loader自动会将数据变成tensor \n",
    "len(loader),next(iter(loader))[0].shape,next(iter(loader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.全连接神经网络 回归任务\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        #一定要写继承\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2,32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,1),\n",
    "            #回归不需要softmax  \n",
    "            #torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "model = Model()\n",
    "\n",
    "model(torch.randn(8,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.23614206910133362\n",
      "20 0.0018427318427711725\n",
      "40 0.0009924585465341806\n",
      "60 5.855775816598907e-05\n",
      "80 8.928721217671409e-05\n"
     ]
    }
   ],
   "source": [
    "# 4.训练\n",
    "model = Model()\n",
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "    #改变一下损失函数\n",
    "    loss_fun = torch.nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        for i,(x,y) in enumerate(loader):\n",
    "            out = model(x)\n",
    "           \n",
    "            loss = loss_fun(out,y)\n",
    "            #标准3布\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch%20 == 0:\n",
    "            #都是训练集的结果，其实acc和loss.item()代表的含义差不多\n",
    "            #acc = (out.argmax(dim=1) == y).sum().item() / len(y)\n",
    "            #换一个指标，直接用loss.item()即可\n",
    "            #item只能取一个元素的\n",
    "            print(epoch,loss.item())\n",
    "    torch.save(model,'./test1_model_regression')\n",
    "\n",
    "#开始训练\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1130],\n",
      "        [0.2979],\n",
      "        [0.2737],\n",
      "        [0.1016],\n",
      "        [0.0681],\n",
      "        [0.1542],\n",
      "        [0.8724],\n",
      "        [0.0081]])\n",
      "tensor([[0.1081],\n",
      "        [0.3056],\n",
      "        [0.2767],\n",
      "        [0.1165],\n",
      "        [0.0708],\n",
      "        [0.1589],\n",
      "        [0.9108],\n",
      "        [0.0096]])\n",
      "tensor([[-0.0049],\n",
      "        [ 0.0077],\n",
      "        [ 0.0030],\n",
      "        [ 0.0149],\n",
      "        [ 0.0027],\n",
      "        [ 0.0047],\n",
      "        [ 0.0384],\n",
      "        [ 0.0015]])\n",
      "tensor([[0.1081, 0.1130],\n",
      "        [0.3056, 0.2979],\n",
      "        [0.2767, 0.2737],\n",
      "        [0.1165, 0.1016],\n",
      "        [0.0708, 0.0681],\n",
      "        [0.1589, 0.1542],\n",
      "        [0.9108, 0.8724],\n",
      "        [0.0096, 0.0081]])\n"
     ]
    }
   ],
   "source": [
    "# 5.测试\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    \n",
    "    #先加载模型，当然要是从头到尾跑的那也可以\n",
    "    model = torch.load('./test1_model_regression')\n",
    "\n",
    "    #进入测试模式\n",
    "    model.eval()\n",
    "\n",
    "    #获取一批数据\n",
    "    x,y = next(iter(loader))\n",
    "    out = model(x)\n",
    "    print(out)\n",
    "    print(y)\n",
    "    print(y-out)\n",
    "    print(torch.cat((y,out),dim=1))\n",
    "    #print(out.argmax(dim=1) == y)\n",
    "    #criterion = torch.nn.CrossEntropyLoss()\n",
    "    #loss = criterion(out, y)\n",
    "    #print(loss.item())\n",
    "    #很奇怪，下面这样连这写是错误的\n",
    "    #print(torch.nn.CrossEntropyLoss(out,y).item())\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
