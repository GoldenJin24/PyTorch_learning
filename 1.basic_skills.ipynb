{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化，索引，切片，维度变化等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2]], dtype=torch.int32)\n",
      "==========\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n",
      "==========\n",
      "tensor([[3.2199e-05, 7.9173e-43, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]])\n",
      "==========\n",
      "tensor([[-0.7430],\n",
      "        [ 0.5931],\n",
      "        [ 0.4765],\n",
      "        [-0.6555],\n",
      "        [-0.1487]])\n",
      "tensor([[0.2861],\n",
      "        [0.4898],\n",
      "        [0.6414],\n",
      "        [0.6103],\n",
      "        [0.6389]])\n",
      "tensor([[5],\n",
      "        [5],\n",
      "        [3],\n",
      "        [1],\n",
      "        [6]])\n"
     ]
    }
   ],
   "source": [
    "# 1.创建，初始化         列表，元组，直接写大部分情况都通用\n",
    "# 1.1有明确的值例如列表，数组\n",
    "# Tensor 和 tensor 的区别\n",
    "x1 = torch.tensor([1,3])\n",
    "x1 = torch.from_numpy(np.array([[1],[2]]))\n",
    "print(x1)\n",
    "print('==========')\n",
    "\n",
    "# 1.2没有明确的值，例如全0，全1，随机数\n",
    "#torch.ones((2,3)) torch.ones([2,3]) torch.ones(2,3) 都可以\n",
    "x1 = torch.ones(2,3,5)\n",
    "#torch.full((2,3),4) 和 torch.full([2,3],4) 都可以\n",
    "x1 = torch.full((2,3),4)\n",
    "x1 = torch.eye(2,5)\n",
    "print(x1)\n",
    "print('==========')\n",
    "\n",
    "# 1.3划一块区域，但是给值\n",
    "print(torch.empty(3,5))\n",
    "print('==========')\n",
    "\n",
    "# 1.4 标准正态分布和均匀分布(0-1)\n",
    "print(torch.randn(5,1))\n",
    "print(torch.rand(5,1))\n",
    "# 1.5 1-9的整数的均匀分布\n",
    "print(torch.randint(1,10,[5,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 5])\n",
      "tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])\n",
      "torch.float32 torch.int64\n",
      "torch.float32 -> torch.int64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 1.6 等差数列  一个是步长，一个是切出来的个数\n",
    "print(torch.arange(0,10,5))\n",
    "print(torch.linspace(0,10,5))\n",
    "# 1.7 格式\n",
    "'''\n",
    "FloatTensor：包含32位浮点数的tensor。\n",
    "DoubleTensor：包含64位浮点数的tensor。\n",
    "HalfTensor：包含16位浮点数的tensor。\n",
    "\n",
    "ByteTensor：包含8位无符号整数的tensor。\n",
    "CharTensor：包含8位有符号整数的tensor。\n",
    "ShortTensor：包含16位有符号整数的tensor。\n",
    "IntTensor：包含32位有符号整数的tensor。\n",
    "LongTensor：包含64位有符号整数的tensor。\n",
    "'''\n",
    "print(torch.FloatTensor([1,3]).dtype,torch.LongTensor((1,3)).dtype)\n",
    "print(torch.Tensor([1,3]).dtype,'->',torch.tensor([1,3]).dtype)\n",
    "# 1.8 转换格式\n",
    "print(torch.Tensor([1,3]).to(torch.int64).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 28, 28])\n",
      "torch.Size([3, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28])\n",
      "tensor(0.2839) torch.float32 torch.Size([])\n",
      "torch.Size([2, 3, 28, 28])\n",
      "torch.Size([2, 3, 28, 28])\n",
      "torch.Size([2, 3, 28, 5])\n",
      "torch.Size([4, 3, 28, 5])\n",
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#2.索引\n",
    "x1 = torch.arange(119.1).reshape(2,3,4,5)\n",
    "x1 = torch.arange(120).reshape(2,3,4,5)\n",
    "x1 = torch.rand(4,3,28,28)\n",
    "print(x1.shape)\n",
    "# 第一张图片\n",
    "print(x1[0].shape)\n",
    "# 第一张图片的第一个通道\n",
    "print(x1[0,0].shape)\n",
    "# 第一张图片的第一个通道的第一行\n",
    "print(x1[0,0,0].shape)\n",
    "# 第一张图片的第一个通道的第一行第一列值\n",
    "print(x1[0,0,0,0],x1[0,0,0,0].dtype,x1[0,0,0,0].shape)\n",
    "\n",
    "#2.2 切片\n",
    "# 第一张到第二张图片\n",
    "print(x1[0:2].shape)\n",
    "# 第一张到第二张图片的第一个通道到第三个通道\n",
    "print(x1[0:2,:3].shape)\n",
    "# 共28行，-5代表23，-5:25左闭右开那就是23 24 (似乎不能从后往前取)\n",
    "print(x1[0:2,:,:,-10:-1:2].shape)\n",
    "# 用...表示省略的冒号\n",
    "print(x1[...,-10:-1:2].shape)\n",
    "print(x1[2,...].shape)\n",
    "print(x1[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 784])\n"
     ]
    }
   ],
   "source": [
    "# 3. 维度变化\n",
    "x1 = torch.randn(4,1,28,28)\n",
    "print(x1.shape)\n",
    "# 展平\n",
    "print(x1.reshape(4,784).shape)\n",
    "# reshape 和 view 几乎一样，区别是reshape会创建新的变量，如果多次调用比较影响内存；\n",
    "# view则是在原有的内存基础上操作，节省内存，但是警惕修改，新手容易改变原始内存但是忘记。所以推荐用reshape（浪费一点也没事）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 7, 1, 8, 1])\n",
      "torch.Size([1, 3, 1, 1, 7, 1, 8, 1])\n",
      "torch.Size([3, 7, 8])\n",
      "torch.Size([1, 3, 1, 7, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "# 3.2 关于插入或者删除一维度\n",
    "x1 = torch.randn(1,3,1,7,1,8,1)\n",
    "print(x1.shape)\n",
    "# 插入维度\n",
    "print(x1.unsqueeze(3).shape)\n",
    "# 删除维度,没有参数输入则默认删除全部一维度的，输入的参数认为是第几维度，如果该维度不是1，则不会修改，不会报错\n",
    "print(x1.squeeze().shape)\n",
    "print(x1.squeeze(4).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 9])\n"
     ]
    }
   ],
   "source": [
    "# 4.重复扩张\n",
    "x1 = torch.randn(2,3)\n",
    "print(x1.repeat(2,3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([3, 1, 5])\n",
      "torch.Size([3, 1, 5])\n",
      "torch.Size([5, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 5.维度交换\n",
    "# t()必须是二维\n",
    "print(torch.randn(2,5).t().shape)\n",
    "\n",
    "# 一次交换两个  0,1和1，0一样的\n",
    "print(torch.randn(1,3,5).transpose(0,1).shape)\n",
    "print(torch.randn(1,3,5).transpose(1,0).shape)\n",
    "\n",
    "# 调整顺序  \n",
    "print(torch.randn(1,3,5).permute(2,1,0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5615],\n",
      "        [-2.3368],\n",
      "        [-0.5857]])\n",
      "torch.Size([3, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [3, 1].  Tensor sizes: [1, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[229], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m((a\u001b[38;5;241m+\u001b[39mb)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [3, 1].  Tensor sizes: [1, 4]"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,1)\n",
    "b = torch.randn(1,4)\n",
    "print(a)\n",
    "print((a+b).shape)\n",
    "#能广播，不一定能手动expand\n",
    "print(b.expand_as(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拼接拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 32, 8])\n",
      "torch.Size([4, 32, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4,32,8)\n",
    "b = torch.randn(5,32,8)\n",
    "# cat 拼接,默认是第一维，不会产生新的维度，只会增加值\n",
    "print(torch.cat([a,b],dim = 0).shape)\n",
    "\n",
    "# stack 拼接,产生了新的维度去区分a，b;可以通过dim控制新加的维度在哪里\n",
    "b = torch.randn(4,32,8)\n",
    "print(torch.stack([a,b],dim = 2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 17, 8])\n",
      "torch.Size([4, 15, 8])\n",
      "torch.Size([4, 16, 8])\n",
      "torch.Size([4, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "# 拆分，一般通过索引来拆\n",
    "# split 默认第一个维度   返回值是列表 更灵活\n",
    "_1,_2 = a.split([17,15],dim=1)\n",
    "print(_1.shape)\n",
    "print(_2.shape)\n",
    "# chunk 平均分，其实和split差不多，split还能干跟多事情  返回值是元组\n",
    "_1,_2 = a.chunk(2,dim=1)\n",
    "print(_1.shape)\n",
    "print(_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5090, -0.6345, -1.3535,  2.0995],\n",
      "        [ 0.8193,  0.5941,  0.6109,  0.5633],\n",
      "        [ 0.7911,  0.8714,  0.4251,  1.6769],\n",
      "        [ 2.2145, -1.7836,  0.9237,  0.1002]]) torch.Size([4, 4]) torch.Size([4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0534,  0.0045, -0.2588,  0.3932],\n",
       "        [ 1.8882,  2.0330, -0.7434, -0.6472],\n",
       "        [ 0.2641,  0.2667, -1.0224,  2.2177],\n",
       "        [-0.2302, -0.0354,  0.3402, -0.6421]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 线性代数中的矩阵乘法  @ 和matmul一样\n",
    "a = torch.randn(4,4)\n",
    "b = torch.randn(4,4)\n",
    "print(a @ b,(a @ b).shape,a.matmul(b).shape)\n",
    "#哈达玛积 保证形状一样直接每个数和每个数相乘\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7723,     nan,  0.3049,     nan],\n",
      "        [ 0.1982,     nan,  0.1217,  0.4273],\n",
      "        [-0.7382,     nan,  0.3830,  0.3714],\n",
      "        [    nan, -0.5076, -0.8135,  0.7047]])\n",
      "tensor([[-2.5569,     nan,  0.4399,     nan],\n",
      "        [ 0.2860,     nan,  0.1756,  0.6165],\n",
      "        [-1.0650,     nan,  0.5526,  0.5358],\n",
      "        [    nan, -0.7323, -1.1736,  1.0167]])\n",
      "tensor([[1.1852, 0.9938, 3.8827, 0.6098],\n",
      "        [3.3847, 0.5013, 3.0940, 4.6329],\n",
      "        [1.6128, 0.5233, 4.3349, 4.2622],\n",
      "        [0.6701, 1.8256, 1.5579, 7.5632]])\n",
      "tensor([[1.0000, 0.9938, 1.0000, 0.6098],\n",
      "        [1.0000, 0.5013, 1.0000, 1.0000],\n",
      "        [1.0000, 0.5233, 1.0000, 1.0000],\n",
      "        [0.6701, 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# log 和 ln 和 exp\n",
    "print(a.log())\n",
    "print(a.log2())\n",
    "print(a.exp())\n",
    "print(a.exp().clamp(0,1))\n",
    "\n",
    "# 向下取整 floor  向上取整 ceil 四舍五入 round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 属性统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 3., 5.],\n",
      "        [2., 4., 6.]])\n",
      "tensor(6.)\n",
      "tensor(21.)\n",
      "tensor(720.)\n",
      "tensor(5)\n",
      "tensor(0)\n",
      "======\n",
      "tensor([1, 1, 1])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1,3,5],[2,4,6]])\n",
    "print(a)\n",
    "print(a.max())\n",
    "print(a.sum())\n",
    "print(a.prod())\n",
    "#最大值下标， 最小值下标  单个数字索引   没有告诉具体的位置  ；常用的话还是要告诉求行的最大，列的最大这样比较好\n",
    "print(a.argmax())\n",
    "print(a.argmin())\n",
    "print('======')\n",
    "print(a.argmax(dim=0))\n",
    "print(a.argmin(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5394) tensor(21.) tensor(9.5394) tensor([2.2361, 5.0000, 7.8102])\n"
     ]
    }
   ],
   "source": [
    "# 范数norm  默认的是第二范数，同时可以指定维度\n",
    "print(a.norm(),a.norm(1),a.norm(2),a.norm(2,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[2., 4., 6.],\n",
      "        [1., 3., 5.]]),\n",
      "indices=tensor([[1, 1, 1],\n",
      "        [0, 0, 0]])) \n",
      " torch.return_types.topk(\n",
      "values=tensor([[5., 3.],\n",
      "        [6., 4.]]),\n",
      "indices=tensor([[2, 1],\n",
      "        [2, 1]]))\n",
      "torch.return_types.kthvalue(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1])) torch.return_types.kthvalue(\n",
      "values=tensor([2., 4., 6.]),\n",
      "indices=tensor([1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# topk问题\n",
    "print(a.topk(2,dim=0),'\\n',a.topk(2,dim=1))\n",
    "\n",
    "# 第二个小的值\n",
    "print(a.kthvalue(2),a.kthvalue(2,dim=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
